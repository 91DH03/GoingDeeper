{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-9. 프로젝트: Vocabulary Size를 변경해서 시도해보기\n",
    "지금까지는 모델을 변경하고, 모델을 조합해서 성능을 올리는 일에 힘썼습니다. 그런데 어쩌면 성능을 높이는 방법은 단순히 모델을 조정하는 일에 한정되지 않을 수 있습니다. 데이터의 전처리는 모델의 성능에 영향을 직접적으로 줍니다. 특히나 Bag of Words를 기반으로 하는 DTM이나 TF-IDF의 경우, 사용하는 단어의 수를 어떻게 결정하느냐에 따라서 성능에 영향을 줄 수 있겠죠.\n",
    "\n",
    "중요도가 낮은 단어들까지 포함해 너무 많은 단어를 사용하는 경우에도 성능이 저하될 수 있고, 반대로 너무 적은 단어들을 사용해도 성능이 저하될 수 있습니다. 이렇게 변화된 단어의 수는 또 어떤 모델을 사용하느냐에 따라 유리할 수도, 불리할 수도 있습니다.\n",
    "\n",
    "단어의 수에 따라서 모델의 성능이 어떻게 변하는지 테스트해 봅시다.\n",
    "```\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n",
    "```\n",
    "앞서 num_words로 사용할 단어의 수를 조정할 수 있다는 것을 배웠습니다. 빈도수가 많은 순서대로 나열했을 때, num_words의 인자로 준 정숫값만큼의 단어를 사용하고 나머지 단어는 전부 <unk>로 처리하는 원리였었죠.\n",
    "\n",
    "아래의 두 가지 경우에 대해서 지금까지 사용했던 모델들의 정확도를 직접 확인해 보세요.\n",
    "    \n",
    "**1. 모든 단어 사용**\n",
    "```   \n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "```    \n",
    "**2. 빈도수 상위 5,000개의 단어만 사용**\n",
    "```    \n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)\n",
    "```\n",
    "**3. 직접 단어 개수를 설정해서 사용**\n",
    " \n",
    "위 단계에서 5000으로 제시된 num_words를 다양하게 바꾸어 가며 성능을 확인해보세요. 변화된 단어 수에 따른 모델의 성능을 연구해 보세요. 최소 3가지 경우 이상을 실험해 보기를 권합니다.\n",
    "\n",
    ">**사용할 모델**         \n",
    "나이브 베이즈 분류기, CNB, 로지스틱 회귀, 서포트 벡터 머신, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리, 보팅\n",
    "\n",
    "**4. 딥러닝 모델과 비교해 보기**\n",
    "    \n",
    "위 과정을 통해 나온 최적의 모델과 단어 수 조건에서, 본인이 선택한 다른 모델을 적용한 결과와 비교해 봅시다. 감정 분석 등에 사용했던 RNN이나 1-D CNN 등의 딥러닝 모델 중 하나를 선택해서 오늘 사용했던 데이터셋을 학습해 보고 나오는 결과를 비교해 봅시다. 단, 공정한 비교를 위해 이때 Word2Vec 등의 pretrained model은 사용하지 않도록 합니다.\n",
    "\n",
    "\n",
    "| 평가문항 | 상세기준 |\n",
    "| --- | --- |\n",
    "| 1. 분류 모델의 accuracy가 기준 이상 높게 나왔는가? | 3가지 단어 개수에 대해 8가지 머신러닝 기법을 적용하여 그중 최적의 솔루션을 도출하였다. |\n",
    "| 2. 분류 모델의 F1 score가 기준 이상 높게 나왔는가? | Vocabulary size에 따른 각 머신러닝 모델의 성능변화 추이를 살피고, 해당 머신러닝 알고리즘의 특성에 근거해 원인을 분석하였다. |\n",
    "| 3. 딥러닝 모델을 활용해 성능이 비교 및 확인되었는가? | 동일한 데이터셋과 전처리 조건으로 딥러닝 모델의 성능과 비교하여 결과에 따른 원인을 분석하였다. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IbsKUW37z0RY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 / 분석 및 전처리\n",
    "#### 1. 모든 단어 사용\n",
    "- 실험을 위해 데이터셋의 모든 단어를 사용한다.\n",
    "- 해당 데이터셋은 정수 시퀀스로 변환되어 제공된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qRYYfEZj1Oqn"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eocMF0dQ1x8W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 0s 0us/step\n",
      "2121728/2110848 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_gNEuKDx1x-s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 수: 8982\n",
      "테스트 샘플의 수: 2246\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플의 수: {}'.format(len(x_train)))\n",
    "print('테스트 샘플의 수: {}'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H8xAzLXA1yBI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스의 수 : 46\n"
     ]
    }
   ],
   "source": [
    "num_classes = max(y_train) + 1\n",
    "print('클래스의 수 : {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ThxE3b_f1yDe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스의 최대 길이 :2376\n",
      "훈련용 뉴스의 평균 길이 :145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('훈련용 뉴스의 최대 길이 :{}'.format(max(len(l) for l in x_train)))\n",
    "print('훈련용 뉴스의 평균 길이 :{}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-_ytgnvv14cv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 0s 0us/step\n",
      "565248/550378 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0, 1, 2번에 \\, \\, \\ 토큰이 매핑되어있다.\n",
    "- 이를 위해 index에 3을 더해주는 추가 작업이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cd-Cc-b514aY"
   },
   "outputs": [],
   "source": [
    "index_to_word = {index + 3 : word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3HbtSHYy14YD"
   },
   "outputs": [],
   "source": [
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "  index_to_word[index]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B7hOTVra14Vv",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 27595,\n",
       " 28842,\n",
       " 8,\n",
       " 43,\n",
       " 10,\n",
       " 447,\n",
       " 5,\n",
       " 25,\n",
       " 207,\n",
       " 270,\n",
       " 5,\n",
       " 3095,\n",
       " 111,\n",
       " 16,\n",
       " 369,\n",
       " 186,\n",
       " 90,\n",
       " 67,\n",
       " 7,\n",
       " 89,\n",
       " 5,\n",
       " 19,\n",
       " 102,\n",
       " 6,\n",
       " 19,\n",
       " 124,\n",
       " 15,\n",
       " 90,\n",
       " 67,\n",
       " 84,\n",
       " 22,\n",
       " 482,\n",
       " 26,\n",
       " 7,\n",
       " 48,\n",
       " 4,\n",
       " 49,\n",
       " 8,\n",
       " 864,\n",
       " 39,\n",
       " 209,\n",
       " 154,\n",
       " 6,\n",
       " 151,\n",
       " 6,\n",
       " 83,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 155,\n",
       " 11,\n",
       " 15,\n",
       " 7,\n",
       " 48,\n",
       " 9,\n",
       " 4579,\n",
       " 1005,\n",
       " 504,\n",
       " 6,\n",
       " 258,\n",
       " 6,\n",
       " 272,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 134,\n",
       " 44,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 8,\n",
       " 197,\n",
       " 1245,\n",
       " 90,\n",
       " 67,\n",
       " 52,\n",
       " 29,\n",
       " 209,\n",
       " 30,\n",
       " 32,\n",
       " 132,\n",
       " 6,\n",
       " 109,\n",
       " 15,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0RHqY8ZL14TU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_to_word[index] for index in x_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dMMR1eqK14RF"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "J_9SzotV14O2"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mZL40EMg14Mh"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IRPOgEYS14KC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 26506)\n"
     ]
    }
   ],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "print(x_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z4TRWN4w14Hs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 26506)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "print(tfidfv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 머신러닝 모델 학습\n",
    "- 나이브 베이즈 분류기\n",
    "- Complement Naive Bayes Classifier, CNB\n",
    "- 로지스틱 회귀\n",
    "- 서포트 벡터 머신(SVM)\n",
    "- 결정 트리\n",
    "- 랜덤 포레스트\n",
    "- 그래디언트 부스팅 트리\n",
    "- 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wTlFfyvD1367"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2B16rjlg2H-z"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "s40Z51AD2H8c"
   },
   "outputs": [],
   "source": [
    "x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-e6M0KZX2H5_"
   },
   "outputs": [],
   "source": [
    "def train_ml(tfidfv, y_train, tfidfv_test, y_test):\n",
    "    # 나이브 베이즈 분류기 \n",
    "    mod = MultinomialNB()\n",
    "    mod.fit(tfidfv, y_train)\n",
    "    \n",
    "    mod_predicted = mod.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"나이브 베이즈 정확도:\", accuracy_score(y_test, mod_predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    # CNB\n",
    "    cb = ComplementNB()\n",
    "    cb.fit(tfidfv, y_train)\n",
    "    \n",
    "    cb_predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"CNB 정확도:\", accuracy_score(y_test, cb_predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    # 로지스틱 회귀 \n",
    "    lr = LogisticRegression(C=10000, penalty='l2')\n",
    "    lr.fit(tfidfv, y_train)\n",
    "\n",
    "    lr_predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"로지스틱 회귀 정확도:\", accuracy_score(y_test, lr_predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    # 선형 서포트 벡터 머신 \n",
    "    lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "    lsvc.fit(tfidfv, y_train)\n",
    "    \n",
    "    lsvc_predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"SVM 정확도:\", accuracy_score(y_test, lsvc_predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    # 결정 트리(Decision Tree) \n",
    "    tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "    tree.fit(tfidfv, y_train)\n",
    "    \n",
    "    tree_predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"결정 트리 정확도:\", accuracy_score(y_test, tree_predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    # 랜덤 포레스트(Random Forest)\n",
    "    forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "    forest.fit(tfidfv, y_train)\n",
    "    \n",
    "    forest_predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"랜덤 포레스트 정확도:\", accuracy_score(y_test, forest_predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    # 그래디언트 부스팅 트리(GradientBoostingClassifier)\n",
    "    grbt = GradientBoostingClassifier(random_state=0, verbose=3) # verbose=3\n",
    "    grbt.fit(tfidfv, y_train)\n",
    "\n",
    "    grbt_predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"그래디언트 부스팅 트리 정확도:\", accuracy_score(y_test, grbt_predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    # 보팅(Voting)\n",
    "    voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "        ], voting='soft', n_jobs=-1)\n",
    "    voting_classifier.fit(tfidfv, y_train)\n",
    "    \n",
    "    voting_classifier_predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"보팅 정확도:\", accuracy_score(y_test, voting_classifier_predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    return mod, cb, lr, lsvc, tree, forest, grbt, voting_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fb691sS2H3x",
    "outputId": "05f31bc1-5a8a-49fa-841c-fd0838eed866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이브 베이즈 정확도: 0.5997328584149599\n",
      "CNB 정확도: 0.7649154051647373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱 회귀 정확도: 0.813446126447017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 정확도: 0.7782724844167409\n",
      "결정 트리 정확도: 0.6211041852181657\n",
      "랜덤 포레스트 정확도: 0.6544968833481746\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.4301           15.47m\n",
      "         2       76760.8864           15.49m\n",
      "         3   766490025.2967           15.39m\n",
      "         4 660857139232122368.0000           15.25m\n",
      "         5 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           15.09m\n",
      "         6 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           14.93m\n",
      "         7 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           14.80m\n",
      "         8 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           14.66m\n",
      "         9 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           14.53m\n",
      "        10 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           14.38m\n",
      "        11 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           14.22m\n",
      "        12 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           14.06m\n",
      "        13 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           13.89m\n",
      "        14 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           13.74m\n",
      "        15 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           13.59m\n",
      "        16 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           13.42m\n",
      "        17 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           13.27m\n",
      "        18 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           13.12m\n",
      "        19 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           12.96m\n",
      "        20 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           12.79m\n",
      "        21 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           12.61m\n",
      "        22 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           12.45m\n",
      "        23 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           12.28m\n",
      "        24 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           12.11m\n",
      "        25 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.95m\n",
      "        26 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.79m\n",
      "        27 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.62m\n",
      "        28 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.45m\n",
      "        29 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.29m\n",
      "        30 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.13m\n",
      "        31 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.96m\n",
      "        32 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.80m\n",
      "        33 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.63m\n",
      "        34 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.47m\n",
      "        35 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.31m\n",
      "        36 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.16m\n",
      "        37 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.00m\n",
      "        38 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.83m\n",
      "        39 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.68m\n",
      "        40 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.52m\n",
      "        41 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.35m\n",
      "        42 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.19m\n",
      "        43 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.03m\n",
      "        44 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.87m\n",
      "        45 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.71m\n",
      "        46 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.55m\n",
      "        47 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.39m\n",
      "        48 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.23m\n",
      "        49 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.07m\n",
      "        50 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.91m\n",
      "        51 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.75m\n",
      "        52 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.59m\n",
      "        53 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.43m\n",
      "        54 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.27m\n",
      "        55 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.12m\n",
      "        56 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.96m\n",
      "        57 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.80m\n",
      "        58 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.64m\n",
      "        59 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.48m\n",
      "        60 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.32m\n",
      "        61 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.16m\n",
      "        62 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.00m\n",
      "        63 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.84m\n",
      "        64 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.68m\n",
      "        65 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.52m\n",
      "        66 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.37m\n",
      "        67 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.21m\n",
      "        68 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.05m\n",
      "        69 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.89m\n",
      "        70 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.73m\n",
      "        71 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.57m\n",
      "        72 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.42m\n",
      "        73 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.26m\n",
      "        74 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.10m\n",
      "        75 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.94m\n",
      "        76 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.78m\n",
      "        77 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.63m\n",
      "        78 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.47m\n",
      "        79 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.31m\n",
      "        80 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.15m\n",
      "        81 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.99m\n",
      "        82 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.84m\n",
      "        83 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.68m\n",
      "        84 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.52m\n",
      "        85 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.36m\n",
      "        86 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.21m\n",
      "        87 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.05m\n",
      "        88 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.89m\n",
      "        89 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.73m\n",
      "        90 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.57m\n",
      "        91 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.42m\n",
      "        92 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.26m\n",
      "        93 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.10m\n",
      "        94 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           56.66s\n",
      "        95 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           47.21s\n",
      "        96 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           37.77s\n",
      "        97 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           28.33s\n",
      "        98 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           18.88s\n",
      "        99 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.44s\n",
      "       100 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            0.00s\n",
      "그래디언트 부스팅 트리 정확도: 0.7702582368655387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보팅 정확도: 0.8187889581478184\n"
     ]
    }
   ],
   "source": [
    "mod, cb, lr, lsvc, tree, forest, grbt, voting_classifier = train_ml(tfidfv, y_train, tfidfv_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 단어 5,000개 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0wpHwyye2H1W"
   },
   "outputs": [],
   "source": [
    "(x_train_5k, y_train_5k), (x_test_5k, y_test_5k) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hRjvdnP32Hy6"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_5k)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_5k[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_5k = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Go6pPhwt2Hwn"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_test_5k)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_5k[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_5k = decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬을 변환 시켜준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bik8QkCd2U0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 4867)\n"
     ]
    }
   ],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm_5k = dtmvector.fit_transform(x_train_5k)\n",
    "print(x_train_dtm_5k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jjxKzR922Uyf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 4867)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv_5k = tfidf_transformer.fit_transform(x_train_dtm_5k)\n",
    "print(tfidfv_5k.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CWBS9CJS2UwO"
   },
   "outputs": [],
   "source": [
    "x_test_dtm_5k = dtmvector.transform(x_test_5k) # 테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test_5k = tfidf_transformer.transform(x_test_dtm_5k) # DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6b8ARLoS2Uty"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이브 베이즈 정확도: 0.6731967943009796\n",
      "CNB 정확도: 0.7707034728406055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱 회귀 정확도: 0.8058771148708815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 정확도: 0.7689225289403384\n",
      "결정 트리 정확도: 0.6179875333926982\n",
      "랜덤 포레스트 정확도: 0.701246660730187\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.4697           13.98m\n",
      "         2     2131099.0239           13.91m\n",
      "         3 113352934366748750033493137947714414201794552363528671409179356940992512.0000           13.83m\n",
      "         4 645005367195105573487290209089081189090170195029150070319832395667206596719018218881024.0000           13.74m\n",
      "         5 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.58m\n",
      "         6 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.42m\n",
      "         7 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.26m\n",
      "         8 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.11m\n",
      "         9 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           12.97m\n",
      "        10 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           12.82m\n",
      "        11 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           12.70m\n",
      "        12 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           12.56m\n",
      "        13 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           12.42m\n",
      "        14 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           12.28m\n",
      "        15 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           12.14m\n",
      "        16 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           12.00m\n",
      "        17 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           11.85m\n",
      "        18 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           11.71m\n",
      "        19 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           11.56m\n",
      "        20 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           11.41m\n",
      "        21 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           11.27m\n",
      "        22 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           11.13m\n",
      "        23 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           10.99m\n",
      "        24 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           10.85m\n",
      "        25 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           10.70m\n",
      "        26 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           10.56m\n",
      "        27 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.41m\n",
      "        28 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.27m\n",
      "        29 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.13m\n",
      "        30 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.99m\n",
      "        31 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.84m\n",
      "        32 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.71m\n",
      "        33 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.57m\n",
      "        34 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.43m\n",
      "        35 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.28m\n",
      "        36 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.14m\n",
      "        37 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.00m\n",
      "        38 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.86m\n",
      "        39 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.71m\n",
      "        40 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.57m\n",
      "        41 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.43m\n",
      "        42 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.28m\n",
      "        43 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.14m\n",
      "        44 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.00m\n",
      "        45 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.85m\n",
      "        46 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.71m\n",
      "        47 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.57m\n",
      "        48 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.43m\n",
      "        49 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.28m\n",
      "        50 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.14m\n",
      "        51 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.00m\n",
      "        52 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.86m\n",
      "        53 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.71m\n",
      "        54 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.57m\n",
      "        55 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.43m\n",
      "        56 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.28m\n",
      "        57 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.14m\n",
      "        58 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.00m\n",
      "        59 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.86m\n",
      "        60 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.71m\n",
      "        61 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.57m\n",
      "        62 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.43m\n",
      "        63 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.28m\n",
      "        64 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.14m\n",
      "        65 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.00m\n",
      "        66 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.86m\n",
      "        67 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.72m\n",
      "        68 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.58m\n",
      "        69 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.45m\n",
      "        70 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.31m\n",
      "        71 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.17m\n",
      "        72 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.03m\n",
      "        73 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.89m\n",
      "        74 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.75m\n",
      "        75 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.61m\n",
      "        76 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.47m\n",
      "        77 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.32m\n",
      "        78 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.18m\n",
      "        79 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.04m\n",
      "        80 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.89m\n",
      "        81 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.75m\n",
      "        82 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.60m\n",
      "        83 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.46m\n",
      "        84 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.32m\n",
      "        85 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.17m\n",
      "        86 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.03m\n",
      "        87 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.88m\n",
      "        88 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.74m\n",
      "        89 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.59m\n",
      "        90 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.45m\n",
      "        91 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.30m\n",
      "        92 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.16m\n",
      "        93 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.01m\n",
      "        94 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           52.19s\n",
      "        95 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           43.50s\n",
      "        96 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           34.80s\n",
      "        97 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           26.10s\n",
      "        98 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           17.41s\n",
      "        99 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.71s\n",
      "       100 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            0.00s\n",
      "그래디언트 부스팅 트리 정확도: 0.767586821015138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보팅 정확도: 0.8161175422974176\n",
      "time : 1816.326699256897\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time() \n",
    "\n",
    "mod_5k, cb_5k, lr_5k, lsvc_5k, tree_5k, forest_5k, grbt_5k, voting_classifier_5k = train_ml(tfidfv_5k, y_train_5k, tfidfv_test_5k, y_test_5k)\n",
    "\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 단어 1,000개 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CaqtHD7S2Urd"
   },
   "outputs": [],
   "source": [
    "(x_train_1k, y_train_1k), (x_test_1k, y_test_1k) = reuters.load_data(num_words=1000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "60O2EOIS2UpL"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_1k)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_1k[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_1k = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3P48NSqS2Um1"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_test_1k)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_1k[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_1k = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yfk2RLHu2UfC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 969)\n"
     ]
    }
   ],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm_1k = dtmvector.fit_transform(x_train_1k)\n",
    "print(x_train_dtm_1k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "IsUyTad02bTa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 969)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv_1k = tfidf_transformer.fit_transform(x_train_dtm_1k)\n",
    "print(tfidfv_1k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "CJE3qvc92bRO"
   },
   "outputs": [],
   "source": [
    "x_test_dtm_1k = dtmvector.transform(x_test_1k) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test_1k = tfidf_transformer.transform(x_test_dtm_1k) #DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "kJBRK7FA2bOh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이브 베이즈 정확도: 0.6856634016028496\n",
      "CNB 정확도: 0.7346393588601959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱 회귀 정확도: 0.780053428317008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 정확도: 0.7150489759572574\n",
      "결정 트리 정확도: 0.6179875333926982\n",
      "랜덤 포레스트 정확도: 0.707479964381122\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.5717           11.37m\n",
      "         2        3967.8426           11.36m\n",
      "         3        4754.6181           11.25m\n",
      "         4     1016060.1416           11.16m\n",
      "         5     1016079.3289           11.04m\n",
      "         6     6974797.9520           10.93m\n",
      "         7     6974845.8927           10.82m\n",
      "         8     6974845.8761           10.70m\n",
      "         9 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.60m\n",
      "        10 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.49m\n",
      "        11 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.37m\n",
      "        12 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.26m\n",
      "        13 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.14m\n",
      "        14 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.02m\n",
      "        15 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.91m\n",
      "        16 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.79m\n",
      "        17 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.68m\n",
      "        18 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.56m\n",
      "        19 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.44m\n",
      "        20 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.34m\n",
      "        21 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.25m\n",
      "        22 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.16m\n",
      "        23 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.06m\n",
      "        24 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.96m\n",
      "        25 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.87m\n",
      "        26 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.77m\n",
      "        27 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.68m\n",
      "        28 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.58m\n",
      "        29 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.47m\n",
      "        30 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.37m\n",
      "        31 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.26m\n",
      "        32 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.14m\n",
      "        33 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.02m\n",
      "        34 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.90m\n",
      "        35 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.78m\n",
      "        36 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.66m\n",
      "        37 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.54m\n",
      "        38 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.42m\n",
      "        39 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.29m\n",
      "        40 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.17m\n",
      "        41 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.05m\n",
      "        42 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            6.93m\n",
      "        43 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            6.81m\n",
      "        44 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            6.69m\n",
      "        45 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            6.57m\n",
      "        46 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            6.44m\n",
      "        47 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            6.32m\n",
      "        48 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.20m\n",
      "        49 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.08m\n",
      "        50 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.96m\n",
      "        51 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.84m\n",
      "        52 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.72m\n",
      "        53 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.60m\n",
      "        54 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.48m\n",
      "        55 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.36m\n",
      "        56 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.24m\n",
      "        57 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.12m\n",
      "        58 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.00m\n",
      "        59 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.88m\n",
      "        60 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.76m\n",
      "        61 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.64m\n",
      "        62 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.52m\n",
      "        63 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.40m\n",
      "        64 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.28m\n",
      "        65 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.16m\n",
      "        66 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.04m\n",
      "        67 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.92m\n",
      "        68 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.80m\n",
      "        69 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.68m\n",
      "        70 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.56m\n",
      "        71 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.45m\n",
      "        72 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.33m\n",
      "        73 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.21m\n",
      "        74 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.09m\n",
      "        75 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.97m\n",
      "        76 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.85m\n",
      "        77 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.73m\n",
      "        78 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.61m\n",
      "        79 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.49m\n",
      "        80 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.37m\n",
      "        81 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.25m\n",
      "        82 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.14m\n",
      "        83 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.02m\n",
      "        84 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.90m\n",
      "        85 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.78m\n",
      "        86 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.66m\n",
      "        87 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.54m\n",
      "        88 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.42m\n",
      "        89 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.30m\n",
      "        90 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.18m\n",
      "        91 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.07m\n",
      "        92 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           56.89s\n",
      "        93 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           49.80s\n",
      "        94 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           42.69s\n",
      "        95 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           35.58s\n",
      "        96 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           28.48s\n",
      "        97 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           21.37s\n",
      "        98 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           14.25s\n",
      "        99 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            7.13s\n",
      "       100 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            0.00s\n",
      "그래디언트 부스팅 트리 정확도: 0.7453250222617988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보팅 정확도: 0.784060552092609\n",
      "run time : 0:24:52\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "mod_1k, cb_1k, lr_1k, lsvc_1k, tree_1k, forest_1k, grbt_1k, voting_classifier_1k = train_ml(tfidfv_1k, y_train_1k, tfidfv_test_1k, y_test_1k)\n",
    "\n",
    "\n",
    "sec = time.time()-start\n",
    "times = str(datetime.timedelta(seconds=sec)).split(\".\")\n",
    "times = times[0]\n",
    "print('run time :', times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 단어 10,000개 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "80JJ9Gpy2bMI"
   },
   "outputs": [],
   "source": [
    "(x_train_10k, y_train_10k), (x_test_10k, y_test_10k) = reuters.load_data(num_words=10000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "bp5crk4r2bJZ"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_10k)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_10k[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_10k = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "wJsR-6ny2bG7"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_test_10k)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_10k[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_10k = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "kNvRPgvhWrWV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n"
     ]
    }
   ],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm_10k = dtmvector.fit_transform(x_train_10k)\n",
    "print(x_train_dtm_10k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "SbI37q3U2bEm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv_10k = tfidf_transformer.fit_transform(x_train_dtm_10k)\n",
    "print(tfidfv_10k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "8wseCOk12bB5"
   },
   "outputs": [],
   "source": [
    "x_test_dtm_10k = dtmvector.transform(x_test_10k) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test_10k = tfidf_transformer.transform(x_test_dtm_10k) #DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkWRipxs2n1L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이브 베이즈 정확도: 0.6567230632235085\n",
      "CNB 정확도: 0.7707034728406055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱 회귀 정확도: 0.8076580587711487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 정확도: 0.7707034728406055\n",
      "결정 트리 정확도: 0.6202137132680321\n",
      "랜덤 포레스트 정확도: 0.674087266251113\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.4608           14.39m\n",
      "         2       95544.1548           14.39m\n",
      "         3      105411.1055           14.31m\n",
      "         4 26490374809120059619893320924222374741943986946048.0000           14.20m\n",
      "         5 3332464259228453694671945105465820387521328203545526380221295913764842145866429631276902168311601749602693928777633481065758720.0000           14.10m\n",
      "         6 3332464259228453694671945105465820387521328203545526380221295913764842145866429631276902168311601749602693928777633481065758720.0000           13.97m\n",
      "         7 3332464259228453694671945105465820387521328203545526380221295913764842145866429631276902168311601749602693928777633481065758720.0000           13.82m\n",
      "         8 3332464259228453694671945105465820387521328203545526380221295913764842145866429631276902168311601749602693928777633481065758720.0000           13.67m\n",
      "         9 3332464259228453694671945105465820387521328203545526380221295913764842145866429631276902168311601749602693928777633481065758720.0000           13.52m\n",
      "        10 3332464259228453694671945105465820387521328203545526380221295913764842145866429631276902168311601749602693928777633481065758720.0000           13.38m\n",
      "        11 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           13.24m\n",
      "        12 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           13.08m\n",
      "        13 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           12.93m\n",
      "        14 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           12.79m\n",
      "        15 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           12.64m\n",
      "        16 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           12.48m\n",
      "        17 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           12.36m\n",
      "        18 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           12.24m\n",
      "        19 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           12.13m\n",
      "        20 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           12.00m\n",
      "        21 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           11.87m\n",
      "        22 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           11.73m\n",
      "        23 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           11.62m\n",
      "        24 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           11.51m\n",
      "        25 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           11.39m\n",
      "        26 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           11.25m\n",
      "        27 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           11.11m\n",
      "        28 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           10.96m\n",
      "        29 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           10.82m\n",
      "        30 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           10.67m\n",
      "        31 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           10.52m\n",
      "        32 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           10.37m\n",
      "        33 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           10.22m\n",
      "        34 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           10.07m\n",
      "        35 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            9.92m\n",
      "        36 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            9.77m\n",
      "        37 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            9.61m\n",
      "        38 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            9.46m\n",
      "        39 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            9.31m\n",
      "        40 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            9.15m\n",
      "        41 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            9.00m\n",
      "        42 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            8.84m\n",
      "        43 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            8.69m\n",
      "        44 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            8.53m\n",
      "        45 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            8.38m\n",
      "        46 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            8.22m\n",
      "        47 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            8.06m\n",
      "        48 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            7.91m\n",
      "        49 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            7.75m\n",
      "        50 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            7.59m\n",
      "        51 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            7.44m\n",
      "        52 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            7.28m\n",
      "        53 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            7.13m\n",
      "        54 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            6.99m\n",
      "        55 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            6.84m\n",
      "        56 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            6.69m\n",
      "        57 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            6.54m\n",
      "        58 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            6.39m\n",
      "        59 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            6.24m\n",
      "        60 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            6.10m\n",
      "        61 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            5.95m\n",
      "        62 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            5.81m\n",
      "        63 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            5.66m\n",
      "        64 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            5.50m\n",
      "        65 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            5.35m\n",
      "        66 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            5.20m\n",
      "        67 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            5.05m\n",
      "        68 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            4.90m\n",
      "        69 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            4.75m\n",
      "        70 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            4.60m\n",
      "        71 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            4.45m\n",
      "        72 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            4.30m\n",
      "        73 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            4.14m\n",
      "        74 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            3.99m\n",
      "        75 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            3.84m\n",
      "        76 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            3.69m\n",
      "        77 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            3.53m\n",
      "        78 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            3.38m\n",
      "        79 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            3.23m\n",
      "        80 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            3.07m\n",
      "        81 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            2.92m\n",
      "        82 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            2.77m\n",
      "        83 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            2.62m\n",
      "        84 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            2.46m\n",
      "        85 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            2.31m\n",
      "        86 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            2.16m\n",
      "        87 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            2.00m\n",
      "        88 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            1.85m\n",
      "        89 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            1.69m\n",
      "        90 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            1.54m\n",
      "        91 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            1.39m\n",
      "        92 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            1.23m\n",
      "        93 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            1.08m\n",
      "        94 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           55.41s\n",
      "        95 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           46.16s\n",
      "        96 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           36.91s\n",
      "        97 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           27.68s\n",
      "        98 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000           18.44s\n",
      "        99 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            9.22s\n",
      "       100 14834291270935598097793813192422686817633454293445808551806972586635235618984506254543094943361504759490214512223227966451089408.0000            0.00s\n",
      "그래디언트 부스팅 트리 정확도: 0.7662511130899377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "mod_10k, cb_10k, lr_10k, lsvc_10k, tree_10k, forest_10k, grbt_10k, voting_classifier_10k = train_ml(tfidfv_10k, y_train_10k, tfidfv_test_10k, y_test_10k)\n",
    "\n",
    "sec = time.time()-start\n",
    "times = str(datetime.timedelta(seconds=sec)).split(\".\")\n",
    "times = times[0]\n",
    "print('run time :', times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvlSMokV2nyv"
   },
   "outputs": [],
   "source": [
    "(x_train_20k, y_train_20k), (x_test_20k, y_test_20k) = reuters.load_data(num_words=20000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vp9Pk4v42nwU"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_20k)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train_20k[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_20k = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KINpwP9S2ntz"
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_test_20k)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test_20k[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_20k = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kssy-_Md2nra"
   },
   "outputs": [],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm_20k = dtmvector.fit_transform(x_train_20k)\n",
    "print(x_train_dtm_20k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35HCJyKn2nox"
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv_20k = tfidf_transformer.fit_transform(x_train_dtm_20k)\n",
    "print(tfidfv_20k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 단어 20,000개 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CioPszV72nmc"
   },
   "outputs": [],
   "source": [
    "x_test_dtm_20k = dtmvector.transform(x_test_20k) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test_20k = tfidf_transformer.transform(x_test_dtm_20k) #DTM을 TF-IDF 행렬로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj1zZvtd2nj4"
   },
   "outputs": [],
   "source": [
    "start = time.time() \n",
    "\n",
    "mod_20k, cb_20k, lr_20k, lsvc_20k, tree_20k, forest_20k, grbt_20k, voting_classifier_20k = train_ml(tfidfv_20k, y_train_20k, tfidfv_test_20k, y_test_20k)\n",
    "\n",
    "sec = time.time()-start\n",
    "times = str(datetime.timedelta(seconds=sec)).split(\".\")\n",
    "times = times[0]\n",
    "print('run time :', times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_t1hzui_2nhh"
   },
   "outputs": [],
   "source": [
    "bayse = [0.5997328584149599, 0.6731967943009796, 0.6856634016028496, 0.6567230632235085, 0.6193232413178985]\n",
    "cnb = [0.7649154051647373, 0.7707034728406055, 0.7346393588601959, 0.7707034728406055, 0.7671415850400712]\n",
    "logistic = [0.813446126447017, 0.8058771148708815, 0.7346393588601959, 0.7707034728406055, 0.7671415850400712]\n",
    "svm = [0.784060552092609, 0.7666963490650045, 0.7172751558325913, 0.7813891362422084, 0.7778272484416741]\n",
    "tree = [0.6211041852181657, 0.6179875333926982, 0.6179875333926982, 0.6202137132680321, 0.6211041852181657]\n",
    "rf = [0.6544968833481746, 0.701246660730187, 0.707479964381122, 0.674087266251113, 0.6714158504007124]\n",
    "gbs = [0.7702582368655387, 0.767586821015138, 0.7457702582368655, 0.7666963490650045, 0.769813000890472]\n",
    "voting = [0.8187889581478184, 0.8161175422974176, 0.7845057880676759, 0.8116651825467498, 0.8178984861976848]\n",
    "\n",
    "vocab_size_list = ['whole', '5k', '1k', '10k', '20k']\n",
    "model_list = [bayse, cnb, logistic, svm, tree, rf, gbs, voting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCT11R3H2nfV"
   },
   "outputs": [],
   "source": [
    "acc_dict = {}\n",
    "for idx, vs in enumerate(vocab_size_list):\n",
    "    acc_list = []\n",
    "    for model in model_list:\n",
    "        acc_list.append(model[idx])\n",
    "    acc_dict[vs] = sum(acc_list)/len(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEpi2Asb2nct"
   },
   "outputs": [],
   "source": [
    "acc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 딥러닝 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4LENTwn2naN"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = 20000\n",
    "word_vector_dim = 32  \n",
    "\n",
    "model_LSTM = keras.Sequential()\n",
    "model_LSTM.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim))\n",
    "model_LSTM.add(keras.layers.LSTM(32))\n",
    "# model_LSTM.add(keras.layers.Dense(128, activation='relu'))  \n",
    "model_LSTM.add(keras.layers.Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOca5zgg2nXx"
   },
   "outputs": [],
   "source": [
    "tfidfv_20k.shape, y_train_20k.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 데이터 실험\n",
    "sparse한 행렬을 dense하게 만들기 위해 두가지 변환으로 실험\n",
    "\n",
    "toarray() : array로 변환(에러메세지에서 추천하는 방법)\n",
    "\n",
    "todense() : Return a dense matrix representation of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew6dnORd2a_M"
   },
   "outputs": [],
   "source": [
    "tfidfv_20k_arr = tfidfv_20k.toarray()\n",
    "tfidfv_20k_dense = tfidfv_20k.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfWzBp9JfJYV"
   },
   "outputs": [],
   "source": [
    "tfidfv_test_20k_arr = tfidfv_test_20k.toarray()\n",
    "tfidfv_test_20k_dense = tfidfv_test_20k.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woXkdCkHfJVq"
   },
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "\n",
    "model_LSTM.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "            \n",
    "epochs=20\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "history_LSTM = model_LSTM.fit(tfidfv_20k_arr,\n",
    "                                y_train_20k,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[es],\n",
    "                                validation_split=0.2,\n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYqVC3AbfJTB"
   },
   "outputs": [],
   "source": [
    "results = model_LSTM.evaluate(tfidfv_test_20k_arr,  y_test_20k, verbose=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WnSM733fJR1"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = 20000\n",
    "word_vector_dim = 32  \n",
    "\n",
    "model_LSTM_dense = keras.Sequential()\n",
    "model_LSTM_dense.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim))\n",
    "model_LSTM_dense.add(keras.layers.LSTM(32))\n",
    "# model_LSTM_dense.add(keras.layers.Dense(128, activation='relu'))  \n",
    "model_LSTM_dense.add(keras.layers.Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "model_LSTM_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDlBifkBfJOt"
   },
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "\n",
    "model_LSTM_dense.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "            \n",
    "epochs=20\n",
    "batch_size=64\n",
    "\n",
    "history_LSTM_dense = model_LSTM_dense.fit(tfidfv_20k_desnse,\n",
    "                                y_train_20k,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[es],\n",
    "                                validation_split=0.2,\n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FUVrKE9fJL7"
   },
   "outputs": [],
   "source": [
    "results_dense = model_LSTM_dense.evaluate(tfidfv_test_20k_dense,  y_test_20k, verbose=2)\n",
    "print(results_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toarray() : [2.4173861999970296, 0.36197686]\n",
    "\n",
    "todense() : [2.415727226625886, 0.36197686]\n",
    "\n",
    "**toarray(), todense()로 학습한 결과 큰 차이가 나지 않았다.**\n",
    "\n",
    "인풋에 대해 sparse함을 확인할 수 있는 scipy의 issparse를 이용해 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQ5sRSOffJJb"
   },
   "outputs": [],
   "source": [
    "sparse.issparse(tfidfv_test_20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh71Dm9jfJHD"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "\n",
    "sparse.issparse(tfidfv_test_20k_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnDJID-YfJEk"
   },
   "outputs": [],
   "source": [
    "sparse.issparse(tfidfv_test_20k_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toarray(), todense() 모두 sparse했던 행렬을 dense하게 변환시킨 것을 확인했다.\n",
    "\n",
    "TF-IDF행렬로 vectorizing한 input을 사용하기 때문에 행렬형태인 todense()의 결과를 모델에 사용하겠다.\n",
    "\n",
    "#### 성능 향상\n",
    "\n",
    "모델의 레이어를 추가하며 accuracy를 향상시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXA9fnDNfJCD"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def rnn(num_layer=8):\n",
    "\n",
    "    model_dense = keras.Sequential()\n",
    "    model_dense.add(keras.layers.Dense(num_layer*4, activation='relu', input_shape=(tfidfv_20k_dense.shape[1],)))  \n",
    "    model_dense.add(keras.layers.Dense(num_layer*4, activation='relu')) \n",
    "    model_dense.add(keras.layers.Dense(num_layer*8, activation='relu')) \n",
    "    model_dense.add(keras.layers.Dense(num_layer*16, activation='relu'))  \n",
    "    model_dense.add(keras.layers.Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "    model_dense.summary()\n",
    "    \n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "\n",
    "    model_dense.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    epochs=20\n",
    "    batch_size=128\n",
    "\n",
    "    history_dense = model_dense.fit(tfidfv_20k_dense,\n",
    "                                    y_train_20k,\n",
    "                                    epochs=epochs,\n",
    "                                    batch_size=batch_size,\n",
    "                                    callbacks=[es],\n",
    "                                    validation_split=0.2,\n",
    "                                    verbose=1)\n",
    "    \n",
    "    results = model_dense.evaluate(tfidfv_test_20k_dense,  y_test_20k, verbose=2)\n",
    "    print(results)\n",
    "    \n",
    "    return history_dense, model_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV1XjhHnfI_d"
   },
   "outputs": [],
   "source": [
    "history_dense, model_dense = rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVNE4RpdfUTE"
   },
   "outputs": [],
   "source": [
    "def visualize_train(train_history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(train_history.history['accuracy'])\n",
    "    plt.plot(train_history.history['val_accuracy'])\n",
    "    plt.title('accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    plt.plot(train_history.history['val_loss'])\n",
    "    plt.title('loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4gbPrHUfUQx"
   },
   "outputs": [],
   "source": [
    "visualize_train(history_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9FCjezofUOb"
   },
   "outputs": [],
   "source": [
    "model_dense.predict(tfidfv_test_20k_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8WjnFOLhgnd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZkQleUMfULw"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(classification_report(y_test_20k, model_dense.predict(tfidfv_test_20k_dense).argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajBHK1SWfUFR"
   },
   "outputs": [],
   "source": [
    "def graph_confusion_matrix(model, x_test, y_test):#, classes_name):\n",
    "    df_cm = pd.DataFrame(confusion_matrix(y_test, model.predict(x_test).argmax(axis=1)))#, index=classes_name, columns=classes_name)\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=12)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=12)\n",
    "    plt.ylabel('label')\n",
    "    plt.xlabel('predicted value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SornmVjfUCy"
   },
   "outputs": [],
   "source": [
    "graph_confusion_matrix(model_dense, tfidfv_test_20k_dense, y_test_20k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- 시간이 너무 오래 걸려서 훈련을 다 시키지는 못했다.. 계속 돌려보아야겠다.\n",
    "- 평가기준을 위해서 단어 크기 별 정확도를 계산하여 파라미터를 설정할 수 있었다.\n",
    "\n",
    "# 느낀점\n",
    "- 진행흐름의 과정을 함수로 만들어서 시도하여서 코드가 깔끔했으며 여러 실험을 해볼 수 있었다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GD-04",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
